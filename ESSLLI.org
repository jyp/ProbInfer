#+LATEX_COMPILER: xelatex 
#+LATEX_HEADER: %include polycode.fmt
#+LATEX_HEADER: %format . = "."
#+LATEX_HEADER: %format <$> = "{\mathbin{<\!\!\!\$\!\!\!>}}"

# Fallback:
#+LaTeX_HEADER: \DeclareMathOperator*{\SumInt}{\sum}
# https://tex.stackexchange.com/questions/68351/what-is-the-command-for-a-sum-symbol-superimposed-on-an-integral-sign

#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usetikzlibrary{calc}
#+LaTeX_HEADER: \usetikzlibrary{fadings}
#+LaTeX_HEADER: \usetikzlibrary{arrows,automata}
#+LaTeX_HEADER: \usetikzlibrary{intersections}
#+LaTeX_HEADER: \usepackage{listings}
#+LaTeX_HEADER: \usepackage{comment}
#+LaTeX_HEADER: \usepackage{unicode-math}
#+LaTeX_HEADER: \newcommand\measure[1]{\mathsf{measure}(#1)}
#+LaTeX_HEADER: \newcommand\Li{\ensuremath{{L}}}
#+LaTeX_HEADER: \newcommand\Spk{\ensuremath{{S}}}
#+LaTeX_HEADER: \usepackage{fontspec}
#+LaTeX_HEADER: \setmainfont{Libertinus Serif}
#+LaTeX_HEADER: \setsansfont{Libertinus Sans}
#+LaTeX_HEADER: \setmathfont{Libertinus Math}
#+LaTeX_HEADER: \usepackage{stackengine}
#+LaTeX_HEADER: \DeclareMathOperator*{\Sumint}{\ensurestackMath{\stackinset{c}{}{c}{}{\displaystyle\sum}{\stackanchor[0pt]{\symbol{"2320}}{\symbol{"2321}}}}}
# Ugly in display mode: \DeclareMathOperator*{\SumInt}{ \mathchoice {\ooalign{$\displaystyle\sum$\cr\hidewidth$\displaystyle\int$\hidewidth\cr}} {\ooalign{\raisebox{.14\height}{\scalebox{.7}{$\textstyle\sum$}}\cr\hidewidth$\textstyle\int$\hidewidth\cr}} {\ooalign{\raisebox{.2\height}{\scalebox{.6}{$\scriptstyle\sum$}}\cr$\scriptstyle\int$\cr}} {\ooalign{\raisebox{.2\height}{\scalebox{.6}{$\scriptstyle\sum$}}\cr$\scriptstyle\int$\cr}}}
# +STARTUP: latexpreview # super slow
#+LaTeX_HEADER: \usepackage{svg}

#+TITLE: A probabilistic semantics for natural language
#+AUTHOR: Jean-Philippe Bernardy and Aleksandre Maskharashvili
#+DATE: 32nd European Summer School in Logic, Language and Information, Week 2: 9‚Äì13 Aug 2021



Preamble: finding the learning material



- Clone the repository: ~git clone https://github.com/jyp/ProbInfer; cd ProbInfer~ (The lecture notes are in ~ESSLLI.org~)
- Install the dependencies using nix (see also https://nixos.org/guides/install-nix.html)

#+begin_src bash
  sh <(curl -L https://nixos.org/nix/install) --daemon
  nix-shell
#+end_src

- Load an example: ~ghci DiceExample.hs~
- run: ~doit~
  
* Inference under uncertainty: Problem description
Link to the video lecture: https://youtu.be/3zQVGohcVd4

  At least since Aristotle, reasoning with and within natural language
  has been subject of studies. Traditionally, many of these studies
  were focused on identifying a logical apparatus of human reasoning.
  More recently, various logic based frameworks where proposed to study
  various aspects of human reasoning, including formalizing notions of
  knowledge, beliefs, hypothesis, making new conclusions based on
  that, updating knowledge base, etc.

  Together with logical aspects, human linguistic activity as well
  involves reasoning with and about incomplete, uncertain, vague, implicit
  information. This includes making inferences, drawing conclusions 
  with uncertain information at hand. 
  
** Logic as the means of categorical judgments/inferences

   - Aristotelian Syllogisms
   - limitations of Aristotelian Syllogisms for reasoning
   - (see also ‚Äúall men are mortal‚Äù below)

*** Aristotelian Syllogisms

    #+begin_example
  ______________________   ____________________
  |Every dog barks.    |   | No cat barks.    | 
  |Goofy is a dog.     |   | Tom is a cat.    |  
  |)===>               |   | )===>            |
  |Goofy barks.        |   | Tom doesn't bark |
  |____________________|   |__________________|
    #+end_example

What comes after the ~)===>~ symbol can be inferred from what is
before. Propositions before the symbol ~)===>~ are called /premises/,
and the statement after ~)===>~ is usually called /the conclusion/ 
(note that there can be many or even no premise, but there is one and only one
conclusion). We may say that the conclusion follows or is entailed from the premises.

#+begin_example
  Every dog barks. ----CONTRARY---- No dog barks.
    |           \                /         | 
    |            \              /          |
    |             \            /           |
    |              \          /            |
    |               \        /             |
  S |                \      /CONTRADICTION | S
  U |                 \    /    (Negation) | U
  B |                  \  /                | B
  A |                   \/                 | A
  L |                   /\                 | L
  T |                  /  \                | T 
  E |                 /    \               | E
  R |                /      \CONTRADICTION | R
  N |               /        \ (Negation)  | N
    |              /          \            |
    |             /            \           |
    |            /              \          |
    v           /                \         v
  Some dog barks. ----CONTRARY---- Some dog doesn't bark.
#+end_example

   We can do a bit more in the Aristotelian settings:

    #+begin_example
  ___________________________________________________
  |Every cat is a mammal.  | No insect barks.       |
  |Every mammal eats.      | Every fly is an insect.|
  |)===>                   | )===>                  |
  |Every cat eats.         | No fly barks           |
  |________________________|________________________|
    #+end_example

  
  In short, Aristotelian syllogisms concern with universally quantified
  statements, their subalterns, and negations. 
  
*** First-Order Predicate Logic 

  In mathematical logics, First-Order Logic (FOL, Predicate Calculus) can
  be seen as a mathematical heir of the Aristotelian syllogisms. 
  Introduced by Gottlob Frege in the end of 19th Century, FOL makes use of:
  - logical connectives (&, ‚à®, ‚Üí, ¬¨)
  - constants and variables (mary, john, 1, 0, 2, x, y, etc.)
  - n-ary predicates (WOMAN, MAN, WALK, HUG, MEET, SEND, LOVE, etc.)
  - n-ary functional symbols (+, -, etc.)
  - quantifiers ‚àÄx and ‚àÉx, for every variable x
  
  - A ~term~ is obtained by applying n-ary function to constants,
  variables, and/or other terms, which are n in total.
  - An ~atomic formula~ is an n-ary predicate whose all places are occupied by
  terms, i.e., it is of shape P(t1,...,tn), where P is an n-ary predicate symbol 
  and t1,...,tn are terms.
  - A ~literal (aka molecule)~ is an atomic formula or its negation, that is, 
  it is of shape P(t1,...,tn) or ¬¨P(t1,...,tn) where P is an n-ary predicate 
  symbol and t1,...,tn are terms. 
  - A ~formula~ is either:
                       - a literal 
                       - application of logical connectives to formulae
                       - application of a quantifier to a formula: ‚àÄx(A) or ‚àÉx(A)

  
  | Pluto is a pet                | PET(pluto)                           |
  | Goofy hugs Pluto              | HUG(goofy, pluto)                    |
  | Pluto is a dog and a star     | DOG(pluto) & STAR(pluto)             |
  | If Goofy is late, Pluto barks | LATE(goofy) ‚Üí BARK(pluto)            |
  | Pluto is not mortal           | ¬¨MORTAL(pluto)                       |
  | Either Pluto or Goofy barked  | BARK(pluto) ‚à® BARK(goofy)            |
  | A man walks                   | ‚àÉx(MAN(x) & WALK(x))                 |
  | Every man walks               | ‚àÄx(MAN(x) ‚Üí WALK(x))                 |
  | Every woman saw Joe           | ‚àÄx(WOMAN(x) ‚Üí SEE(x,joe))            |
  | Every woman saw a man         | ‚àÄx(WOMAN(x) ‚Üí ‚àÉy(MAN(y) & SEE(x,y))) |
  |                               | ‚àÉy(MAN(y) & ‚àÄx(WOMAN(x) ‚Üí SEE(x,y))) |

   Example of FOL usage:

   
   #+begin_example
   ____ENGLISH_______________________FOL_TRANSLATION__________
   | Mary is a woman                  WOMAN(mary)            | 
   | Mary walks                       WALK(mary)             |
   | )===>                            )===>                  |
   | A woman walks                    ‚àÉx(WOMAN(x) & WALK(x)) |
   -----------------------------------------------------------
   #+end_example
   How to prove ‚àÉx(WOMAN(x) & WALK(x)) from the premises WOMAN(mary) and WALK(mary)?
   In FOL, it's quite natural to have the following rule: 
   If A(k) holds, then ‚àÉx(A(x)) also holds (where k is a term and x is not free in A(k)).
   By assuming WOMAN(mary) and WALK(mary) hold,
   then (WOMAN(mary) & WALK(mary)) holds as well. 
   In this latter formula, we can apply the FOL rule for ‚àÉ, we get:
   ‚àÉx(WOMAN(x) & WALK(x)).  
                                 Q.E.D.
   
   
   
#   Exercise: Show the following holds (use any rules you would like to use): 
#         #+begin_example

#   ____ENGLISH_______________________FOL_TRANSLATION___________
#   | Every pet sleeps.            ‚àÄx(PET(x) ‚Üí SLEEP(x))       |
#   | Every dog is a pet.          ‚àÄx(DOG(x) ‚Üí PET(x))         |
#   | Pluto is a lovely dog.       LOVELY(pluto) & DOG(pluto)  |  
#   | )===>                        )===>                       |
#   | Pluto sleeps.                SLEEP(pluto)                |
#   |__________________________________________________________|
#     #+end_example

# Hint:  
#   For any formulae œÜ and œà, it holds: œÜ & œà ‚Üí œÜ and œÜ & œà ‚Üí œà  
#   Given that  (œà ‚Üí œÜ) & (œÜ ‚Üí Œ∂), it follows that œà ‚Üí Œ∂  
#   For any term k, ‚àÄx(A(x)) entails A(k) 
#   Given that ‚àÄx(A(x)) and ‚àÄx(B(x)), we can conclude ‚àÄx(A(x) & B(x))


  

** Inferences that are non-categorical (non-logical inferences)

  Not all inferences are logical though. Consider the following
  inference problems:

  | Teddy just got home.                      | $‚ä®$? Teddy is hungry.            |
  | Teddy just drank several glasses of milk. | $‚ä®$? Teddy is not hungry.        |
  | Teddy loves cars.                         | $‚ä®$? Teddy drives a car.         |
  | Teddy hugged a cat.                       | $‚ä®$? Teddy got a cat.            |
  | Teddy enjoys listening to Led Zeppelin.   | $‚ä®$? Teddy loves English bands.  |
  | Teddy is 18 months old.                   | $‚ä®$? Teddy weights around 15 kg. |
  | Teddy can mimic dogs and cats.            | $‚ä®$? Teddy can speak Georgian.   |
  |                                           | $‚ä®$? Teddy is a bear.            |

  One can imagine that the above examples, can be justified by using
  common-sense knowledge.  Speakers, in many cases, may make
  extensive use of such knowledge in order to navigate within the
  context, make specific decisions and take respective steps.
  
  The kind of inferences show above are rather hard to formalize in
  any classical deductive systems, because they require internalization a
  large amount of common sense knowledge.

  Here, we limit our discussion to problems that do not require external
  knowledge about the world. And in many of the cases we study, we will
  be concerned with problems which are /similar/ to the Aristotelian square,
  and generalizations thereof. We will use generalized quantifiers and
  inference problems built with them, which we call /probabilistic/
  inference problems.

** A type of non-logical inference: Probabilistic inference

  One of the main questions that we want to address is the following: 
  Are the rules governing reasoning under uncertainty part of the same system
  as the logical rules? Irrespective of their ontological classification,
  those rules are part of human reasoning and are clearly expressed in
  natural languages (like English) with the specific morpho-syntactic
  and/or lexical means.

  We call an inference demonstrative if the premises necessitate the
  conclusion. In other words, a demonstrative inference is any
  inference that is truth-preserving.
  For example, we can say that Aristotelian syllogism are only concerned
  with demonstrative inferences (given that premises are true, the
  conclusion is true).

  If an inference is not demonstrative, we call it non-demonstrative.
  That is, the conclusion is not necessary to hold (be true) given that
  the premises hold. Hence, non-demonstrative inferences are those which
  are not truth-preserving. We are interested in non-demonstrative
  inferences. Of course, many ‚Äúwrong‚Äù (unreasonable, rejectable)
  inferences are non-demonstrative.

  ‚ÄúIf, however, there is any kind of inference whose premises, although
  not necessitating the conclusion, do lend in weight, support it, or
  make it probable, then such inferences possess a certain kind of
  logical rectitude. It is not deductive validity, but it is important
  anyway.‚Äù     /Wesley C. Salmon, The foundations of Scientific Inference/
  
  We will be mostly interested in this /important/ inferences  where premises
  do not necessitate the conclusion but make it /probable/, which we refer to as
  /probabilistic inference/.

  Here, our focus is on sentences which are /similar/ to universally
  quantified ones.  Instead of traditional, logical quantifiers (every,
  no), we have so called vague (or generalized) quantifiers e.g.,
  most, almost every, many, almost none of, etc.

  | Most dogs bark.         | Goofy is a dog.             | $‚ä®$? Goofy barks.       |
  | Almost no cat barks.    | Tom is a cat.               | $‚ä®$? Tom doesn't bark.  |
  | Cats rarely bite.       | Tom is a cat.               | $‚ä®$? Tom doesn't bite.  |
  | Dogs usually are smart. | Pluto is a dog.             | $‚ä®$? Pluto is friendly. |
  | Many cats sleep.        | Tom is a cat.               | $‚ä®$? Tom sleeps.        |
  | Mice are not lovely.    | Micky is an atypical mouse. | $‚ä®$? Micky is lovely.   |
  

** Linguistic phenomena involved in probabilistic inferences

Lexical and morpho-syntactic apparatus in natural languages (like
English) allow us to express expressions that we've been arguing are
probabilistic in nature.

*** Generalized Quantifiers 

    Logic-based deductive inference usually involves quantifiers,
    universal (e.g. every, all, no) and existential (e.g. there is a).
    (Recap: In classical logical frameworks, the universal and
    existential quantifiers are inter-definable. 
    A question to think about: Why is that so?
        
            Hint: 
                 - Every vampire is sleeping \(‚ü∫\) It is not true
                      that there is a vampire who isn't sleeping
                 - A vampire is sleeping \(‚ü∫\) It is not true that
                                          no vampire is sleeping)

    
    Not all quantifiers in natural languages are universal and/or existential.
    Consider: most, few, many, several, almost none of, etc. These
    quantifiers are called generalized quantifiers. They usually give
    rise to information that is vague, uncertain.
    
   - Most doctors are smart.
   - A few countries has surplus of Covid-19 vaccines.
   - Many countries don't have enough Covid-19 vaccines.
   - Almost no vampires are sleeping.
    
   While the above sentences give us certainly rich information about
   the current state of affairs (aka the world), one cannot be
   certain when discussing particular instantiations of these
   sentences. For example:

   - Dracula is a vampire. Is he sleeping? 
   (Probably yes, but one cannot claim it with the full certainty.)

     We can construct various quantified propositions in English
     whose meaning has /probabilistic/ flavor. One way of thinking
     about it is to consider a universally quantified proposition and
     instead of the universal quantifier take some generalized
     quantifier; the resultant proposition would not any more state a
     universal property (e.i., one that applies to every element in a
     domain), but will rather give rise to somewhat /vague/ information.
     
*** Adverbs of Frequency 
    
    Another source of vagueness are adverbs of frequency, e.g, rarely,
    usually, regularly, frequently, several times a year, probably etc. 
    They modify a verb phrase and thus quantify over actions/events that
    take place. They can also cast doubt on categorical information, 
    make it less  certain. 
    Consider the following example:
    
   - Almost all vampires are friendly.
   - Every vampire is probably friendly.

    Are these two equivalent? Maybe! And if yes, then 
   /probably/ and the generalized quantifier /almost all/ have similar semantic effects.

   In the following examples, we combine adverbs of frequency and generalized quantifiers:
   - Birds are usually able to fly.  (Similar to:  Most birds are able to fly.)
   - Mammals can rarely fly.         (Similar to:  Almost no mammal can fly.)
   
   This gives us a basis to treat (at least for purposes of a limited usage) adverbs of
   frequency in a similar manner as generalized quantifiers. (Adverbs
   of frequency can be seen as generalized quantifiers over events.)

*** Graded Adjectives and Comparatives

    Natural language allows us to express various kinds of properties, some of which
    can be characterized in terms of degrees (scale). For example, cold, colder, too cold, etc.
    That is, we can derive from graded (gradable) properties a way of measuring and comparing objects
    that these properties can be applied. 

    #+begin_example
    John is taller than most people.
    -------------------------------
    $‚ä®$? John is tall.
    #+end_example

    #+begin_example
    Few people are taller than John.
    ------------------------------
    $‚ä®$? John is tall.
    #+end_example

    #+begin_example
    NBA players are taller than most people in U.S.
    Few people are NBA players. 
    Muggsy Bogues is an NBA player.
    ---------------------------------------------
    $‚ä®$? Muggsy Bogues is taller than most people in U.S.
    #+end_example


# A total of 4,509 players have played in the NBA.
# Vince Carter has played with/against 40% of all players in the HISTORY of the NBA.
# Muggsy Bogues is 5'3"

* Probability theory, Bayesian Reasoning, Probabilistic programming
Link to Video lecture: https://youtu.be/XyyPeQ37fhc
** Motivational Problem (1)
 You throw two 6-faced dice.
 # Prior

 You observe that the sum is greater than 8
 # Evidence

 What is the probability that the product is greater than 20?
 # Posterior

Solution: file:DiceExample.hs

** Motivational Problem (2)

 Assume a big bag, which you know contains a lot of red and blue
 balls.  The contents of the bag is thoroughly mixed. You do not know
 the proportion of blue and red balls in the bag.
 # Prior

 You pick 4 balls at random, putting each ball back in the bag after
 looking at it. The first three are red, the last one is blue.
 # Evidence

 What is the probability for your next ball-pick to yield a red ball?
 # Posterior

** Elements of Probability theory                                        
*** Concept: Probability distribution
**** Frequency distribution
 Consider a coin, with two faces, nominally labeled ‚Äúheads‚Äù and
 ‚Äútails‚Äù.

 Throw it \(n\) times. Consider what you will get.

 - \( f(Heads) + f(Tails) = n\)

 Consider a die, with 6 faces. Throw it \(n\) times. Consider what you
 will get.

 Let Œ© = {1,2,3,4,5,6}
 We say that Œ© is the probability space of x.

 Note:
 - $\sum_{x:Œ©} 1 = 6$
 - The measure of the space is 6.

# :Sandro: We'd be better to explicitly mention what the notion "x:Œ©" means.  

**** Discrete probability distributions

 One can define the probability of an event P(x) (with x:Œ©) as the
 limit of a frequency distribution /f/ divided by the total number of
 observations of /x/, for the number of observations tending to infinity.

 If the coin is ‚Äúfair‚Äù, we then expect:

 - P(Heads) = 0.5
 - P(Tails) = 0.5

 For any probability distribution \(P\), over a domain \(Œ©\),
 we expect:

 - $‚àë_{x:Œ©} P(x) = 1$


**** Continuous probability distribution
 Later on, we will mostly turn our attention to set of events Œ© which
 are not discrete. For example, instead of considering whether the coin
 falls heads or tails, consider /where/ it ends up falling, for example
 as a pair of coordinates.

 - $Œ© = ‚Ñù¬≤$

 If we'd attempt to use a probability distribution as before, we'd end up
 with P(x) = 0 for every point.
 So in this case, each element of Œ© is assigned not a probability but a
 /probability density/.

**** Continuous probability distribution: properties

 If \(f\) is the /probability density function/ (PDF) of P, the fundamental
 property becomes:

 - $‚à´_{x:Œ©} f(x) dx = 1$

 Remark: we'll almost never care about the value of \(f\) directly; only
 its behavior under integrals. That is, the only valid question to ask
 is the probability of the coin falling ‚Äúwithin an area‚Äù ‚Äî not
 ‚Äúexactly‚Äù at a given point.

*** Notation

In the scientific literature, the \(P(...)\) notation is incredibly overloaded. 
Let us give a number of overloadings, each in terms of the previous one. 

 - If \(C\) is a subset of Œ©, then
    * $P(C) = ‚àë_{x:C} P(x)$ if Œ© is discrete
    * $P(C) = ‚à´_{x:C} PDF(x)$ dx if Œ© is continuous
 - If \(c(x)\) is a condition (Boolean expression),
      - $P(c) = P(\{x ‚àà Œ© ‚à£ c(x)\})$.
   That is, we check the probability of the set of events which makes \(c\) true.

 - If \(e\) is an expression,
    - $P(e == x)$, where x is a distribution which one has to figure out implicitly.

**** Examples
 - \(P(Heads ‚à™ Tails)\)
 - \(P({d > 3 ‚à£ d ‚àà {1,2,3,4,5,6}})\)
 - \(P(d) = 1/6\)
   
*** Dependent and Independent events and variables

   - Two events A and B are called /independent/ events iff. 

     - $P(A ‚àß B) = P(A) ¬∑ P(B)$.

   - The probability  \(P(A ‚à© B)\) is *not* equal to \(P(A) ¬∑ P(B)\) in general!

*** Examples
**** Coins
 - $P(Heads ‚àß Tails) = 0$
    (Indeed, the events are /dependent/ on each other)

**** Dice

 - Throw a pair of 6-faced dice d‚ÇÅ, d‚ÇÇ. P(d‚ÇÅ+d‚ÇÇ > 9) = ?

    |   || 1 | 2 | 3 |  4 |  5 |  6 |
    |---++---+---+---+----+----+----|
    | 1 || 2 | 3 | 4 |  5 |  6 |  7 |
    | 2 || 3 | 4 | 5 |  6 |  7 |  8 |
    | 3 || 4 | 5 | 6 |  7 |  8 |  9 |
    | 4 || 5 | 6 | 7 |  8 |  9 | 10 |
    | 5 || 6 | 7 | 8 |  9 | 10 | 11 |
    | 6 || 7 | 8 | 9 | 10 | 11 | 12 |

  - 36 (equally probable, aka equiprobable) cases
  - 21 out of 36 satisfy the condition
  - ‚Üí \(P(d‚ÇÅ+d‚ÇÇ > 9) = 21/36 = 7/12\)

*** Conditional probability (1)

 Definition:

 - $P(A ‚à£ B) = P(A ‚àß B) / P(B)$
   -   if $P(B) > 0$

 Example:


 \begin{align*}
      P (Heads ‚à£ Tails) & = P (Heads ‚àß Tails) / P(Head) \\
                        & = 0 / 0.5 \\
                        & = 0
 \end{align*}
 \begin{align*}
      P (d‚ÇÅ+d‚ÇÇ > 6 ‚à£ d‚ÇÇ=5) & = P(d‚ÇÅ+d‚ÇÇ > 6 ‚àß d‚ÇÇ=5) / P(d‚ÇÇ = 5) \\
                           & =  (5 / 36)           / (1/6) \\
                           & =   5 / 6
 \end{align*}

*** Conditional probability (2)

 Alternatively one can use \(P(A ‚à£ B)\) as a primitive notion and define

 -  $P(A ‚àß B) = P(A ‚à£ B) ¬∑ P(B)$

 This equation is useful when \(P(A ‚à£ B)\) is known or easy to compute.

**** Example

     \begin{align*}
     P(d‚ÇÅ+d‚ÇÇ > 6 ‚àß d‚ÇÇ=5)  
       & = P (d‚ÇÅ+d‚ÇÇ > 6 ‚à£ d‚ÇÇ=5) ¬∑ P(d‚ÇÇ=5)  & \text{by the above}  \\
       & = P (d‚ÇÅ+5 > 6)  ¬∑ P(d‚ÇÇ=5)         & \text{by substitution}  \\
       & = P (d‚ÇÅ > 1)  ¬∑ P(d‚ÇÇ=5)           & \text{by subtracting 5}  \\
       & = (5/6)       ¬∑ (1/6)  \\
       & = (5/6)       . (1/6)  \\
       & = 5/36 
     \end{align*}

*** Probability Laws
**** Probability of disjoint events

 A and B are said to be disjoint (as sets or conditions) iff. 
   -  $A ‚à© B = ‚àÖ$
   -  $A ‚àß B = false$

 If \(A\) and \(B\) are disjoint, then the probability of the union is the sum
 of probabilities:

   - If $A ‚àß B = false$, then $P(A ‚à® B) = P(A) + P(B)$
   - If $A ‚à© B = ‚àÖ$,     then $P(A ‚à™ B) = P(A) + P(B)$

 Remark: Do not confuse ‚Äúdisjoint‚Äù and ‚Äúindependent‚Äù.

# If A and B are disjoint, i.e. P(A ‚à© B)=0, so, if P(A) and P(B) are more than zero, 
# then P(A ‚à© B) is not equal to P(A) ¬∑ P(B) .

**** Probability of negation/complement

 - $P(¬¨A) + P(A) = P(¬¨A ‚à® A) = P(true) = 1$

 hence:
 - $P(¬¨A) = 1 - P(A)$

 exercise: use sets instead of Boolean expressions.

**** Law of total probability
 if \(B‚ÇÅ\), \(B‚ÇÇ\) disjoint and \(B‚ÇÅ ‚à® B‚ÇÇ = true\):

 - \(P(A) = P(A ‚àß B‚ÇÅ) + P(A ‚àß B‚ÇÇ)\)

 Indeed, 

\begin{align*} 
 P(A ‚àß B‚ÇÇ) + P(A ‚àß B‚ÇÅ)
  & = P((A ‚àß B‚ÇÇ) ‚à® (A ‚àß B‚ÇÅ))  \text{disjoint events} \\
  & = P(A ‚àß (B‚ÇÇ ‚à® B‚ÇÅ)) \\
  & = P(A ‚àß true) \\
  & = P(A) \\
\end{align*} 

**** Probability of disjunction

 What if \(A\) and \(B\) are not disjoint? 

 - \(P(A ‚à® B) = P (A) + P(B) - P(A ‚àß B)\)

 Lemma: if \(A ‚äÜ B\) then \(P(A) + P(B ‚àñ A) = P(B)\)
 Proof:

 \begin{align*}
  P(A) + P(B ‚àñ A)  & = P (A ‚à™ (B ‚àñ A))   & \text{ disjoint events} \\
                   & = P (B)
 \end{align*}

 Proof of theorem:
 \begin{align*}
   P(A ‚à™ B) & = P(A ‚à™ (B ‚àñ A)) \\
            & = P(A) + P(B ‚àñ A))       & \text{disjoint events} \\
            & = P(A) + P(B) - P(A‚à©B) & \text{Lemma}
 \end{align*}

**** Summary of Laws

    - $P(Œ©) = 1$
    - $P(¬¨ A) = 1 - P(A)$
    - $P(A ‚à® B) = P (A) + P(B) - P(A‚àßB)$
    - $P(A ‚àß B) = P(A ‚à£ B) ¬∑ P(B)$
    - if $B‚ÇÅ, B‚ÇÇ$ complementary, $P(A) = P(A ‚àß B‚ÇÅ) + P(A ‚àß B‚ÇÇ)$

*** Random variables with priors (discrete)
 How to evaluate \(P(A)\), for an expression \(A\) depending on a random
 variable \(r\)?

 Using the law of total probability:

 \begin{align*}
 P(A) & = ‚àë_{i:Œ©} P(r=i ‚àß A) \\
      & = ‚àë_{i:Œ©} P(A ‚à£ r=i) P(r=i) \\
 \end{align*}

 We can even write
 - $P(A)  = ‚àë_{i:Œ©} P(A[i/r]) P(r=i)$

 Writing \(A[i/r]\) to mean that we substitute \(r\) for \(i\) in the expression \(A\).
 But \(A[i/r]\) no longer depends on a random variable. (It is either true
 or false). So it is less confusing to write \(Indicator\) instead of \(P\),
 where \(Indicator(c) = 1\) when \(c\) is true and 0 when \(c\) is false.

 - $P(A)  = ‚àë_{i:Œ©} Indicator(A[i/r]) P(r=i)$

 In such an equation, we can call \(P(r=i)\) the prior probability of \(r=i\).

**** Example (discrete)
 In the dice example, every time that we want to evaluate the
 probability of an event (or condition) A which *depends* on the roll
 of the dice, we can use the formula:

 -  \(P(A) = ‚àë_{i‚àà[1..6]} ‚àë_{j‚àà[1..6]} P(A ‚à£ d‚ÇÅ = i ‚àß d‚ÇÇ = j) P(d‚ÇÅ = i ‚àß d‚ÇÇ = j)\)

 If the dice are fair and independent, then \(P(d‚ÇÅ = i ‚àß d‚ÇÇ = j) = 1/36\),
 for any \(i,j\), and we have:

 -  \(P(A) = ‚àë_{i‚àà[1..6]} ‚àë_{j‚àà[1..6]} P(A ‚à£ d‚ÇÅ = i ‚àß d‚ÇÇ = j) / 36\)

 and even:
 -  \(P(A) = ‚àë_{i‚àà[1..6]} ‚àë_{j‚àà[1..6]} Indicator(A[d‚ÇÅ = i,d‚ÇÇ = j]) / 36\)

 If the dice were unfair or dependent, we'd change the prior 
 $P(d‚ÇÅ = i ‚àß d‚ÇÇ = j)$ accordingly.

 Say if \(A\) is \(d‚ÇÅ+d‚ÇÇ > 6\):


 -  P(d‚ÇÅ+d‚ÇÇ>6) = ‚àë(i‚àà[1..6]) ‚àë(j‚àà[1..6]) Indicator(i+j > 6) / 36

**** Random variables with priors (continuous)
 For continuous variables, we have:

 \begin{align*}
   P(A) & = ‚à´_{x:Œ©} f(r=i ‚àß A) f·µ£(x) dx \\
        & = ‚à´_{x:Œ©} Indicator(A[x/r]) f·µ£(x) dx
 \end{align*}

 with: f·µ£ the PDF of the distribution of the random variable r.

 Example: probability that the coin falls on the table:
 Let 
   - $A      ‚âú (x ‚àà Table)$   
     (where Table is a subset of $‚Ñù¬≤$ representing the surface of the table.)
   - $f(Coin) ‚âú 1/a$  
     (using a simple model where I can throw the coin anywhere in the room and a = room areaRoom area.)

      \begin{align*}
          P(A) & = ‚à´_{x:‚Ñù¬≤} Indicator(x‚ààTable) \frac 1 a dx \\
               & = \frac 1 a ‚à´_{x:‚Ñù¬≤} Indicator(x‚ààTable) dx \\
               & = \frac 1 a \left(‚à´_{x‚ààTable} Indicator(x‚ààTable) dx + ‚à´_{x‚àà(‚Ñù¬≤ ‚àñ Table)} Indicator(x‚ààTable) dx\right) \\
               & = \frac 1 a \left(‚à´_{x‚ààTable} 1 dx + ‚à´_{x‚àà(‚Ñù¬≤ ‚àñ Table)} 0 dx\right) \\
               & = \frac 1 a \left(1 ‚à´_{x‚ààTable} dx + 0 ‚à´_{x‚àà(‚Ñù¬≤ ‚àñ Table)} dx\right) \\
               & = \frac 1 a \left(‚à´_{x‚ààTable} dx\right) \\
               & = \frac 1 a t \\
               & = \frac t a
      \end{align*}

 Any idea of a better model? What would be the effect on the outcome?
*** Evidence and posteriors
 Assume now that we have some *evidence* to account for.

 In the case of the dice, we could somehow know that the sum is
 greater than 8. Then what is the *posterior* probability that the
 product is less than 20?

 - $E ‚âú d‚ÇÅ+d‚ÇÇ > 8$
 - $A ‚âú d‚ÇÅ ¬∑ d‚ÇÇ > 20$

 We need to account for \(E\):
 - \(P(A ‚à£ E) = P (A ‚àß E) / P(E)\)

 -  \begin{multline*}
       P(d‚ÇÅ¬∑d‚ÇÇ < 20 ‚àß d‚ÇÅ+d‚ÇÇ > 8) = \\
          ‚àë_{i‚àà[1..6]} ‚àë_{j‚àà[1..6]} Indicator(i+j > 8 ‚àß i¬∑j > 20) P(d‚ÇÅ = i ‚àß d‚ÇÇ = j)
    \end{multline*}

** Probabilistic Programs
 The above gives an informal recipe to compute probabilities. It works
 for simple problems, but it's easy to make mistakes when tackling
 non-trivial problems.  We set out to make the process systematic --
 and so it can also be the basis of complex models. This systematic
 approach will help with the interpretation of natural language, which
 is our real goal.

 We are really interested in defining spaces of possible situations.
 We will do so with the help of /probabilistic
 programs/. Probabilistic programs are procedures whose return value
 may depend on sampling from a distribution.

 Besides, the \(P(...)\) notation is a problem on its own, with so much
 overloading that it's often hard to grasp. Probabilistic programs
 largely eliminate issues of the \(P(...)\) notation.

*** Probability distributions (1)
 The basic characteristic of probabilistic programs is the ability to
 sample from probability distributions. We will list and discuss some
 of them.

 - \(DiscreteUniform(Œ©)\)
   \begin{align*}
       P(x) & = 1 / \measure {Œ©}   & \text{if~} x ‚àà Œ© \\
            & = 0             & \text{otherwise}
   \end{align*}
   Suitable if all choices are equally probable --- for a finite set of events.

 - \(Uniform(a,b)\)
   - PDF(x) = 1/(b-a)   if x ‚àà [a,b]
            = 0         otherwise
   - if all choices are equally probable --- if the set of events is continuous and bounded.

 - \(Bernoulli(p)\)
   - \(P(0) = 1-p\)
   - \(P(1) = p\)
   - Two choices, which are not necessarily equally probable.
   - In our example, we can represent the space of Balls by Bernoulli(œÅ)

In probabilistic programs, we can sample from a distribution using a special-purpose primitive |sample|. Example:
 
#+begin_src haskell
 ballBlue = sample (Bernoulli œÅ)
#+end_src

(The proportion of balls in the bag is \(œÅ\) and is unknown)

*** Probability distributions (2)
 - \(Normal(Œº,œÉ)\)
   - PDF(x) = ${\displaystyle {\frac {1}{\sqrt {2\pi \sigma ^{2}}}}e^{-{\frac {(x-\mu )^{2}}{2\sigma ^{2}}}}}$
   - Often used to model a random variable which depends itself on many variables in an unknown way
   - mean = Œº

 - \(Beta(Œ±,Œ≤)\) with  \(Œ±,Œ≤ > 0\)
   - $\displaystyle { PDF(x) = \frac {x^{\alpha -1}(1-x)^{\beta -1}}{\mathrm {B} (\alpha ,\beta )}}$ if \(x ‚àà [0,1]\), \(0\) otherwise

     where ${\displaystyle \mathrm {B} (\alpha ,\beta )={\frac {\Gamma (\alpha )\Gamma (\beta )}{\Gamma (\alpha +\beta )}}}$  and \(Œì\) is the Gamma function. (just a normalization factor)

   - Useful to model bounded variables, with non-uniform distributions.
   - \(Beta(1,1) = Uniform[0,1]\)
   - Mean = $\frac {Œ±} {Œ±+Œ≤}$
   - increasing \(Œ±\) ‚Äúpushes‚Äù the distribution towards 1; \(Œ≤\) towards 0.

*** Constants

    A simple (but very important!) probabilistic program is the one
    which just returns a constant $k$. We write it:


    #+begin_src haskell
return k
    #+end_src

*** Sequencing instructions

If |t| and |u| are probabilistic probabilistic programs, then the
following is also a probabilistic program:

#+begin_src haskell
do x ‚Üê t; u
#+end_src

Here, we additionally allow the rest of the program, |u|, to depend on
(use) the variable |x|.
Note that this is the /only construction/ that can declare a variable.


#+begin_src haskell
ball = do isBlue ‚Üê sample (Bernoulli œÅ);
          return (if isBlue then Blue else Red)
#+end_src


*** Observations

 To represent evidence, we introduce the program |observe(œÜ)|, where œÜ
 is a Boolean-valued expression. If œÜ is true, then |observe(œÜ)| has
 no effect.  If œÜ is false, then |observe(œÜ)| then the program is
 aborted; in essence the samples made above in the program are discarded.
 We will make formally precise later.
 
 In our running example, a program sampling a blue ball can be written as:

 #+begin_src haskell
 blueBall = do
   x ‚Üê ball
   observe (x == Blue)
   return x
 #+end_src

A program sampling two balls, and at least one blue, is:

 #+begin_src haskell
 twoBallsAtLeastOneBlue = do
   x ‚Üê ball
   y ‚Üê ball
   observe (x == Blue || y == Blue)
   return (x,y)
 #+end_src

*** Expected truth value (aka ‚ÄúProbability‚Äù)

 We can now conveniently phrase our problems in this framework:

 If we let |die = sample (DiscreteUniform [1..6])|

 The program representing situations where the sum of dice is \(> 8\) is :

#+begin: example-src :filename "DiceExample.hs" :defn twoDieAbove8
twoDieAbove8 = do
  d‚ÇÅ ‚Üê die
  d‚ÇÇ ‚Üê die
  observe (d‚ÇÅ + d‚ÇÇ > 8)
  return (d‚ÇÅ,d‚ÇÇ)
#+end:

What want to do now is to sample a pair of dice using the above
procedure, then evaluate the probability that the product is greater
than 20.

 Given a random pair \((x,y)\) sampled by |twoDieAbove6|, we'd be interested in the
 truth value of the proposition

   -  \(œÜ = x √ó y > 20\)

 But œÜ depends on which pair \((x,y)\) we choose. So the /probability/ of œÜ is
 given by the expected value of the indicator function \(Indicator(œÜ)\).

 So we could define the probability of œÜ as:

\(ùîº_{[(x,y)‚ààtwoDieAbove8]}(Indicator(x√óy > 20))\)


There is however a convenient way to represent the above expression in
terms of a probabilistic program directly:

#+begin: example-src :filename "DiceExample.hs" :defn problem1
problem1 = do
  (x,y) ‚Üê twoDieAbove8
  return (indicator <$> (x*y > 20))
#+end:

This way, to evaluate probabilities, the only thing that we need is to
take the expected value of probabilistic programs. (We will see
later how to do this.)

**** Remark (skip)

#+begin_src haskell
do x ‚Üê a
   observe (b x)
   return (c x)
#+end_src

is not the same as

#+begin_src haskell
do x ‚Üê a
   return (b x ‚ü∂ c x)
#+end_src

 In the first instance, if \(b(x)\) is false so \(x\) is not
 counted. In the 2nd program if \(b(x)\) is false it is counted as
 satisfying the condition.

*** Example: drug test (Wikipedia)
 #+begin_quote
 Suppose that a test for using a particular drug is 99% sensitive and
 99% specific. That is, the test will produce 99% true positive results
 for drug users and 99% true negative results for non-drug
 users. Suppose that 0.5% of people are users of the drug. What is the
 probability that a randomly selected individual with a positive test
 is a drug user?
 #+end_quote

This can be modeled by the following probabilistic program:

#+begin: example-src :filename "DrugTest.hs" :defn exampleDrug
exampleDrug = do
  -- prior
  isUser ‚Üê sample (Bernoulli (0.5 * percent))
  -- evidence
  testedPositive ‚Üê if_ isUser $ \case
    True ‚Üí sample (Bernoulli (99 * percent))
    False ‚Üí sample (Bernoulli (1 * percent))
  observe testedPositive
  -- posterior
  return isUser
#+end

(Complete program file here: file:DrugTest.hs)

*** Answer To Introductory Problem

 [[*Motivational Problem (2)][link back to the introductory problem]]

One might think that a simple answer is \(\frac 3 4\). But is this
correct?  Let's try to use the concepts developed so far and write a
probabilistic program modeling the problem:

#+begin: example-src :filename "Balls.hs" :defn exampleBalls
exampleBalls = do
  -- a priori distribution of the proportion of blue balls.
  œÅ ‚Üê sample (Uniform 0 1) -- œÅ ‚Üê sample (Beta 0.5 0.5) -- alternative
  -- sample a ball in the bag:
  let ball = do
        x ‚Üê sample (Bernoulli œÅ)
        return (boolToColor <$> x) 
  -- sample a red ball:
  let redBall = do
        b ‚Üê ball  -- take a ball
        observe (testEq b Red) -- if it is not red, forget this situation.
  -- sample a blue ball:
  let blueBall = do
        b ‚Üê ball
        observe (testEq b Blue)
  redBall
  redBall
  redBall
  blueBall
  x ‚Üê ball
  return x
  where boolToColor :: Bool ‚Üí Color
        boolToColor = \case
            True ‚Üí Blue
            False ‚Üí Red
#+end:

What do you think is the expected result of this program? This is the
topic of the rest of the lecture.

*** Meaning of probabilistic programs

Intuitively, probabilistic programs define distributions. However,
defining distributions directly poses a number of technical
problems. So instead we define the related notion of integrator.

We define the /integrator/ of \(f(z)\) over a probabilistic program
\(t\), ($\Sumint_{z‚ààt} f(z)$) as a generalization of the
integration/summation of \(f(x)\) for the possible return values \(z\)
returned by \(t\).

We define:
 
 \begin{align*}
\Sumint_{z ‚àà return¬†x} f(z) & = f(x) \\
\Sumint_{z ‚àà (do¬†x ‚Üê t; u(x))} f(z) & = \Sumint_{x ‚àà t} \Sumint_{z ‚àà u(x)} f(z) \\
\Sumint_{z ‚àà sample(c)} f(z) & = ‚à´_{x‚àà‚Ñù} \mathrm{PDF}_c(x) ¬∑ f(x) dx & \text{sampling in a continuous distribution}\\
\Sumint_{z ‚àà sample(d)} f(z) & = \sum P_d(x) ¬∑ f(x) & \text{sampling in a discrete distribution}\\
\Sumint_{z ‚àà observe(œÜ)} f(z) & = Indicator (œÜ) ¬∑ f(‚óá)
 \end{align*}


(Note that the ‚Äúobserve‚Äù program does not return any result, so the
 integrand \(f(z)\) cannot in fact depend on \(z\), the result of the
 program.)

Don't worry if you don't get all details at this stage. The main point
is that we can define the meaning of probabilistic programs in a
precise, mathematical manner. (Without referring to how probabilistic
programs are run on an actual machine.)

 
**** Measure

Probabilistic programs do *not* define distributions. That
is, the total /measure/ of a program is not guaranteed to be 1. For example,
the program |observe false| has a measure of 0.
     
We define the measure of a program t as follows:

 $\measure t = \sum_{z‚ààt} 1$

Thus the measure ‚Äúcounts'' every element with the same unit weight.

**** Lemma: integrators are linear operators

 [In the vector space of real-valued functions]    

 Lemma:
 - $\Sumint_{x‚ààt} (k √ó f(x)) = k √ó \Sumint_{x‚ààt} f(x)$
 - $\Sumint_{x‚ààt} (f(x) + g(x)) = \Sumint_{x‚ààt} f(x) + \Sumint_{x‚ààt} g(y)$

 Proof:
 By induction on \(t\), relying on the linearity of $‚àë$ and $‚à´$.

**** Lemma: Properties of measures

 \begin{align*}
   \measure {sample¬†d} & = 1 \\
   \measure {observe¬†œÜ} & = Indicator ‚ü¶œÜ‚üß \\
   \measure {do¬†x‚Üêt;u} & = \Sumint_{x‚ààt} \measure u
 \end{align*}

 - Proposition: For probabilistic program \(t\), $\measure t ‚â§ 1$.

**** Expected value

     
The expected value of $f(z)$ over a value $z$ sampled by a
probabilistic program $t$ is defined as follows:

$\frac {\Sumint_{z‚ààt} f(z)} {\measure t}$

It is also very useful to define the expected value of
a probabilistic program itself, as simply the expected value of its
returned values:

 $ùîº(t) = ùîº_{z‚ààt}[z]$

(This make sense only when \(t\) returns a numerical value.)

**** Expected truth value (aka ‚Äúprobability‚Äù)

If a program \(t\) returns the type |Bool| (either the constant |True|
or |False|), we can define the probability of \(t\) (to return |True|)
as:

\(‚Ñô(t) = ùîº[z‚Üêt;return (Indicator(z))]\)

(So we simply convert the Boolean value to 0 or 1, and then take the
expected value.)

**** Example: Drug test
 Given |exampleDrug| defined as above (file:DrugTest.hs::exampleDrug)


 - Compute: $‚Ñô(exampleDrug)$

 - Answer: $\frac {\Sumint_{z‚ààt} indicator(z)} {\measure {exampleDrug}}$

**** Exercise: compute the above answer using the definitions.

Solution = \(\frac {ok} {total}\)

\begin{align*}
 ok 
 & = \measure t \\
 & =
 ‚àë_{isUser:Bool} Bernoulli(0.005)(isUser) ¬∑
 ‚àë_{testPositive:Bool} Bernoulli(if¬†isUser¬†then¬†0.99¬†else¬†0.01)(testPositive) ¬∑
 Indicator(testPositive) \\
 & =
 ‚àë_{isUser:Bool} Bernoulli(0.005)(isUser) ¬∑
 Bernoulli(if¬†isUser¬†then¬†0.99¬†else¬†0.01)(true)  \\
 & =
 ‚àë_{isUser:Bool} Bernoulli(0.005)(isUser) ¬∑
 if¬†isUser¬†then¬†0.99¬†else¬†0.01 ¬∑ \\
 & =
 Bernoulli(0.005)(false) (if¬†false¬†then¬†0.99¬†else¬†0.01) +
 Bernoulli(0.005)(true)  (if¬†true¬†then¬†0.99¬†else¬†0.01) \\
 & =
 0.995 √ó 0.01 + 0.005 √ó 0.99 \\
 & = 
 0.0149
\end{align*}

 Compute the numerator:

\begin{align*}
 total 
 & =
 ‚àë_{isUser:Bool} Bernoulli(0.005)(isUser) ¬∑
 ‚àë_{testPositive:Bool} Bernoulli(if isUser then 0.99 else 0.01)(testPositive) ¬∑
 Indicator(testPositive) ¬∑
 Indicator(isUser) \\
 & = 
 Bernoulli(0.005)(true) ¬∑
 Bernoulli(if true then 0.99 else 0.01)(true) \\
 & = 
 0.005 √ó
 0.99 \\
 & =
 0.00495 
\end{align*}

 So the ratio is: 0.332214765101

*** Exercise: Evaluating the answer introductory problem

Solution.

 \(\frac {ok} {total}\) with:

\begin{align*}
 ok =
 & ‚à´_{œÅ:[0..1]} dœÅ \\
 & ‚àë_{b1:[0,1]} b(œÅ,b1) ¬∑ \\
 & ‚àë_{b2:[0,1]} b(œÅ,b2) ¬∑ \\
 & ‚àë_{b3:[0,1]} b(œÅ,b3) ¬∑ \\
 & ‚àë_{b4:[0,1]} b(œÅ,b4) ¬∑ \\
 & ‚àë_{b5:[0,1]} b(œÅ,b5) ¬∑ \\
 & (b1 ¬∑ b2 ¬∑ b3 ¬∑ (1-b4) ¬∑ b5)
\end{align*}

\begin{align*}
 total =
 & ‚à´_{œÅ:[0..1]} dœÅ \\
 & ‚àë_{b1:[0,1]} b(œÅ,b1) ¬∑ \\
 & ‚àë_{b2:[0,1]} b(œÅ,b2) ¬∑ \\
 & ‚àë_{b3:[0,1]} b(œÅ,b3) ¬∑ \\
 & ‚àë_{b4:[0,1]} b(œÅ,b4) ¬∑ \\
 & (b1 ¬∑ b2 ¬∑ b3 ¬∑ (1-b4))
\end{align*}



(Computing the integrals is daunting! But can your algebra system do it?)

 Reminder:
 Where: 
  - \(b(œÅ,0) = œÅ\)
  - \(b(œÅ,1) = 1-œÅ\)

*** Final note on Beta distribution. 
 If we observe \(n\) reds and \(m\) blues, the posterior distribution
 for the parameter \(œÅ\) is \(Beta(n+1/2, m+1/2)\).

 In particular, the expected value of this proportion is \(\frac {n+0.5}  {n+m+1}\)

 In our example, we do not expect to a \(3/4\) prediction for
 the ratio of red ball, but rather but \(3.5/5\). (Which is exactly what the
 program predicts!)

*** Exercise: the children problem

Model the following problem:

A friend of yours has exactly two children. One of them is a boy. What
is the probability that the other one is a boy?

Solution: file:Pair.hs

*** Exercise: betting on games

 - Consider the game ‚ÄúSloubi‚Äù.
 - Each player $p$ of Sloubi is assigned a rating $œÅ_p$. The rating is
   intrinsic to each player, and never changes.
 - There is an element of randomness in Sloubi. In any match, $p$ will
   win over $q$ if $œÅ_p > œÅ_q + m$, with $m$ taken in |Normal(0,100)|. 
   (Even worse players will win, sometimes.)
 - Alice wins over Bob, Bob wins over Charles and David. What is
   Alice's probability to win over David in their next game?

**** Solution

     file:Sloubi.hs

* Compositional translation of inference problems into probabilistic programs
Link to Video lecture: https://youtu.be/XJugaNpNi_0

** Montagovian semantics

+ Note: a comprehensive course was given in the first week of ESSLLI
  2021 "Introduction to natural language formal semantics‚Äù, by Ph. de
  Groote and Y. Winter.

As a quick reminder, we can associate types with syntactic categories,
in the following manner:

#+BEGIN_SRC haskell
  type CN = Ind ‚Üí Prop
  type VP = Ind ‚Üí Prop
  type NP = VP ‚Üí Prop
  type Quant = CN ‚Üí NP
  type Ind = ...
#+END_SRC
But what are individuals? It is mysterious!

In fact, Montagovian semantics normally consider Individuals to be
/abstract/. This means that nothing needs to be known about them to be
able to interpret phrases. However, if one needs to give specific
semantics to lexical items (perhaps in specific domains), we need to
get more concrete.

In fact we keep (nearly) all Montagovian semantics as such, and make
certain things concrete.

** Interpreting inference problems: recipe.
 Remember the classic syllogism:

- all men are mortal
- socrates is a man
- socrates is mortal?

It can be interpreted as probabilistic program this way:

#+begin: example-src :filename "LingExamples.hs" :defn exampleSocrates0
exampleSocrates0 = do
  man ‚Üê samplePredicate       -- declare predicate
  mortal ‚Üê samplePredicate    -- declare predicate
  observe (every man mortal)  -- premiss, interpreted using Montegovian semantics
  socrates ‚Üê sampleInd        -- declare individual ("for some random ...")
  observe (man socrates)      -- premiss, interpreted using Montegovian semantics
  return (mortal socrates)    -- conclusion, interpreted using Montegovian semantics
#+end:

That is:
- every time we have a new lexical item, we sample it at random
- every time we have a premiss, we observe it to be true
  + this means that samples which do not satisfy the premiss will be
    rejected
- at the end, we return the truth value of the conclusion
  + so the probability of the program corresponds to the probability
    of entailment.
  

On this example, this means that we quantify |man| over all CNs; so
the program does not have any /a-priori/ notion of what ‚Äúman‚Äù means
--- we sample over the whole space of CNs. The distribution for ‚Äúman‚Äù
gets refined by evidence (in this case ‚Äú‚àÄ(x:man) mortal(x)‚Äù,
‚Äúman(socrates)‚Äù).  The way, the semantics for programs that we gave
mean that all worlds where we can find non mortal men will be filtered
out.

** Interpretation of semantic categories

We still need to chose a definition
for |samplePredicate|, |sampleInd|.

One would expect the above inference regarding Socrates to hold in
every possible world. Consequently, we'd like \(‚Ñô[exampleSocrates0] =
1\)
So, the definitions of \(CN\),\(VP\),\(Ind\), etc. must be
well-chosen so that the above formula evaluates to 1.

The goal is to
 - interpret each syntactic category as a probabilistic program
 - interpret each syntactic operator as a function from/to the
   appropriate spaces.
 - so that we get meaningful inferences

*** Propositions

We'll simply interpret propositions as Boolean-valued expressions.

#+begin: example-src :filename "LingExamples.hs" :defn "type Prop" 
type Prop = Probabilistic Bool
#+end:

*** Individuals
Fortunately we now have a way to interpret individuals as elements in a space.

Examples: 

- multi-variate normal distribution of dimension $k$
  - covariance matrix (?)
- uniform distribution in a box $[0..1]^k$
  
#+begin: example-src :filename "LingExamples.hs" :defn sampleInd
sampleInd = sampleVectorOf (Gaussian 0 1)
#+end:

- Discussion: what would /you/ choose? Why?

This idea is directly inspired from machine learning: individual
(situations) can be represented by a vector.

This is indeed used for:
  - Words
  - Sentences
  - Images

*** Reminder: set cardinalities

If \(card(A) = n\), then \(card(A ‚Üí Bool) = 2^{card({A})}\). So, there are ‚Äúexponentially many‚Äù
more predicates over a set than there are elements in the set.

A related fact is that \(‚Ñï\) is countable, but the set of predicates over
natural numbers \(‚Ñï ‚Üí Bool\) is uncountable.

There is an obvious way to integrate over $[0,1]$, but how to
integrate over $[0,1] ‚Üí Bool$? How to take ‚Äúthe average‚Äù over all possible
predicates?

*** Space of predicates

We're deliberately going to restrict the set of possible predicates to
make our endeavor possible.  Hopefully, it's enough to limit oneself
to a small enough (sampleable) subset and still have a useful model.

If words can be represented by a vector, then so can predicates (hopefully).

(NOTE: other ideas would be to sample from a set of programs which implement predicates.)

*** Idea 1
If an individual is represented by a vector $x$ and a vector $p$
represents a predicate, then $x$ is said to satisfy the predicate if
$p ‚àô x > 0$. (Ie, both vector are oriented in the same direction in
the underlying euclidean space.)


#+begin: example-src :filename "LingExamples.hs" :defn predicateSimple
predicateSimple = do
  v ‚Üê sampleNormedVector
  b ‚Üê sample (Gaussian 0 1)
  return (\x ‚Üí (b + x ¬∑ v) > 0)
#+end:

#+HEADER: :file predicate.svg :imagemagick yes
#+HEADER: :results output silent :headers '("\\usepackage{tikz}")
#+HEADER: :fit yes :imoutoptions -geometry 400 :iminoptions -density 600
#+BEGIN_src latex
\begin{tikzpicture}[scale=3.0]
  \shade [shading=radial] (0,0) circle (1);
\draw[->] (-2,0)--(2,0) node[right]{$x$};
\draw[->] (0,-2)--(0,2) node[above]{$y$};
\clip (-2,-2) rectangle (2,2);
\draw[xshift=15,rotate=25] (0,-2) -- (0,-1) -- (0,-4)  -- (0,4);
\fill[xshift=15,rotate=25,color=blue,opacity=0.1] (0,-4) rectangle +(4,8);
\node at (1, 1) {$\textit{mortals}$};
\end{tikzpicture}
#+END_src

                       [[./predicate.svg]]

*** Idea 2: Boxes


If an individual is represented by a vector $x$ and a pair of vectors
$p$ , $q$ represent a predicate, then $x$ is said to satisfy the predicate
if $x$ is in the box delimited by the corners $p$ and $q$.

   #+begin_src haskell
   predicate = do
       p ‚Üê sampleVectorOf (Gaussian 0 1)
       q ‚Üê sampleVectorOf (Gaussian 0 1)
       return (Œªx. ‚àÄi. p·µ¢ < x·µ¢ < q·µ¢)
   #+end_src

*** Idea 3: Your Idea!

Discussion point
    
*** Common nouns

Common nouns are interpreted as predicates:

\(‚ü¶CN‚üß = Pred\)

Consequently, any given common noun \(cn\) is a predicate. We can also
interpret the common noun \(cn\) as the underlying sub-distribution of
individuals which is filtered by satisfying the predicate associated
with \(cn\), like so.

#+begin: example-src :filename "LingExamples.hs" :defn sampleSome
sampleSome cn = do
  x ‚Üê sampleInd
  observe (cn x)
  return x
#+end:

** Interpretation of semantic operators
*** Generalized quantifiers

 We can defined generalized quantifiers by appealing to the measure of
 probabilistic programs:

   - |atLeast Œ∏ a (Œªx. œÜ) = measure (do x ‚Üê a; observe (œÜ)) > Œ∏ * measure (a)|
   - |atMost  Œ∏ a (Œªx. œÜ) = measure (do x ‚Üê a; observe (œÜ)) < Œ∏ * measure (a)|

 Indeed, the statement |observe(œÜ)| will discard certain samples
 of |x|, and affect the measure in proportion to the probability of |œÜ(x)|
 to hold.
 
 Note that this requires to /evaluate the measure of a program inside a probabilistic program itself/. 

 However so far we have seen only how to evaluate their
 probabilities. No sweat, we can use an alternative definition, as
 follows:
 
 Equivalently:
 #+begin: example-src :filename "LingExamples.hs" :defn atLeast
 atLeast Œ∏ cn vp = probability (do x ‚Üê sampleSome cn; return (vp x)) > Œ∏
 #+end:

 In turn we can define all sorts of generalized quantifiers:

   - |‚ü¶Most cn vp‚üß = atLeast Œ∏ ‚ü¶cn‚üß (Œªx. ‚ü¶vp‚üß(x))|
   - |‚ü¶Few cn vp‚üß = atMost (1-Œ∏) ‚ü¶cn‚üß (Œªx. ‚ü¶vp‚üß(x))|


**** Example:

 - most men are mortal
 - socrates is a man
 - \(‚ä®\)? socrates is mortal

 #+begin: example-src :filename "LingExamples.hs" :defn exampleSocrates
 exampleSocrates = do
   man ‚Üê samplePredicate
   mortal ‚Üê samplePredicate
   observe (most man mortal)
   socrates ‚Üê sampleInd
   observe (man socrates)
   return (mortal socrates)
 #+end:

**** Example:
- Few animals fly.
- Most birds fly.
- Every bird is an animal.
- \(‚ä®\)? most animals are not birds

  
#+begin: example-src :filename "LingExamples.hs" :defn exampleBirds
exampleBirds = do
   fly ‚Üê samplePredicate
   bird ‚Üê samplePredicate
   animal ‚Üê samplePredicate
   observe (most bird fly)
   observe (few animal fly)
   observe (every bird animal)
   return (most animal (\x ‚Üí not <$> (bird x)))
#+end:
 
#+HEADER: :file birds.svg :imagemagick yes
#+HEADER: :results output silent :headers '("\\usepackage{tikz}")
#+HEADER: :fit yes :imoutoptions -geometry 400 :iminoptions -density 600
#+BEGIN_src latex
\begin{tikzpicture}[scale=3.0]
  \shade [shading=radial] (0,0) circle (1);
  \draw[->] (-1.25,0) -- (1.25,0); % node[right] {$x$};
  \draw[->] (0,-1.25) -- (0,1.25); % node[above] {$y$};
  \clip (-1.25,-1.30) rectangle (1.25, 1.25);

  \draw[xshift=15,rotate=15] (0,-2) -- (0,-1) -- (0,1) node[right] {$\mathit{bird}$} -- (0,2);
  \fill[xshift=15,rotate=15,color=blue,opacity=0.15] (0,-2) rectangle +(4,4);

  \draw[yshift=-5,xshift=10,rotate=-10] (0,-2) -- (0,-1) node[right] {$\mathit{fly}$} -- (0,1) -- (0,2);
  \fill[yshift=-5,xshift=10,rotate=-10,color=red,opacity=0.15] (0,-2) rectangle +(4,4);

  \foreach \x/\xtext in {1/1}
    \draw[shift={(\x,0)}] (0pt,2pt) -- (0pt,-2pt) node[below] {$\xtext$};

  \foreach \y/\ytext in {1/1}
    \draw[shift={(0,\y)}] (2pt,0pt) -- (-2pt,0pt) node[left] {$\ytext$};
  \end{tikzpicture}
#+END_src
                       [[./birds.svg]]

Same example, but using boxes for predicates:



#+HEADER: :file birds-box.svg :imagemagick yes
#+HEADER: :results output silent :headers '("\\usepackage{tikz}")
#+HEADER: :fit yes :imoutoptions -geometry 400 :iminoptions -density 600
#+BEGIN_src latex
\begin{tikzpicture}[scale=3.0]
  \draw[->] (-1.25,0) -- (1.25,0); % node[right] {$x$};
  \draw[->] (0,-1.25) -- (0,1.25); % node[above] {$y$};
  \clip (-1.25,-1.30) rectangle (1.25, 1.25);

  \fill [fill=black, fill opacity=0.1] (-1,-1) rectangle (1,1);
  \filldraw [fill=red, fill opacity=0.13]

 (-0.9,-0.4) rectangle (0.65,0.9) node[opacity=1,anchor=north east] {$\mathit{fly}$};
  \filldraw [fill=blue,  fill opacity=0.13] (-0.2,-0.3) rectangle (1,0.5) node[opacity=1,anchor=north east] {$\mathit{bird}$};


  \foreach \x/\xtext in {1/1}
    \draw[shift={(\x,0)}] (0pt,2pt) -- (0pt,-2pt) node[below] {$\xtext$};

  \foreach \y/\ytext in {1/1}
    \draw[shift={(0,\y)}] (2pt,0pt) -- (-2pt,0pt) node[left] {$\ytext$};
  \end{tikzpicture}
#+END_src

                       [[./birds-box.svg]]

**** Choice of Œ∏

 The above depends on a threshold Œ∏ which constitutes the proportion
 from which most/few/etc. begin to hold.
     
 One can choose Œ∏ by studying native speakers. However, one should
 expect that you won't get a single value of Œ∏ which will fit all
 situations, but rather you'll observe a distribution for
 Œ∏. Probabilistic programs are ideally suited to deal with this.

 #+begin: example-src :filename "LingExamples.hs" :defn exampleSocrates2
 exampleSocrates2 = do
   Œ∏ ‚Üê sample (Beta 5 2) -- for example
   man ‚Üê samplePredicate
   mortal ‚Üê samplePredicate
   observe (atLeast Œ∏ man mortal)
   socrates ‚Üê sampleInd
   observe (man socrates)
   return (mortal socrates)
 #+end:

 Below we'll leave Œ∏ abstract.

*** Universal Quantifiers

 We define |forAll a œÜ| as stochastic certainty of |œÜ(x)| for elements
 given by the probabilistic program |a|, namely:

     #+begin_src haskell
     forAll a œÜ = probability (do x ‚Üê a; return (œÜ x)) == 1 
     #+end_src

 Given the above, we can interpret natural language phrases such as
 ‚Äúevery man is mortal‚Äù, as follows:

   - $‚ü¶Every cn vp‚üß = forAll ‚ü¶cn‚üß ‚ü¶vp‚üß$

**** Pitfall (SKIP)

 Assume

  - $t = sample (Uniform [-1..1])$
  - \(œÜ = (x ‚â† 0)\)

 We have:

  - \(\measure{t}              = 2\)
  - \(\measure{x ‚Üê t; observe (œÜ)} = 2\)

 Indeed, we filtered out a single point --- its measure is 0 

 And according to the above definition:

   - \(‚ü¶‚àÄ(x:A). œÜ‚üß = true\)

 (So this operator really means ‚Äúfor stochastically all‚Äù in probabilistic logic)

***** Dealing with this pitfall 

 - attempt to have a precise measure that counts single elements
   * not computable, because HOL is undecidable
 - use ‚Äúsoft transitions‚Äù
   * still does not make \(‚àÄx:A. œÜ\) coincide with the usual definition
     (but can help with the approximation algorithms in many cases.)
 - do not use problematic domains
   * unless otherwise note, this is what we will do.

*** Existential Quantifiers

 One can define existential quantifiers by dualizing universals in
 either version.

 #+begin_src haskell
 exist a œÜ = probability a œÜ > 0
 #+end_src

*** Comparatives
 - Mary is tall
 - John is tall
 - ‚ÄúMary is taller than John‚Äù?


 We can support graded predicates and comparatives. We do so by
 generalizing predicates.

 We define |Grade| to be function from individuals to reals.

#+begin: example-src :filename "LingExamples.hs" :defn "type Grade" 
type Grade = Ind ‚Üí Probabilistic R
#+end:

 If the function evaluates to a positive value for individual \(x\),
 then \(x\) is considered to satisfy the non-scalar retraction of the
 predicate.
 #+begin: example-src :filename "LingExamples.hs" :defn is :include-type t
 is :: Grade ‚Üí Ind ‚Üí Prop
 is g x = g x > 0
 #+end:

 Then one can also compare individuals with respect to any scalar predicate:

 more :: Grade ‚Üí Ind ‚Üí Ind ‚Üí Prop
 more g x y = g x > g y
 #+end:


 Eg. to test |more tall mary john|, we check if the 'tallness' of mary
 is greater than that of john.

**** Idea 1

 The expression $b + d ¬∑ x$ can be interpreted as a degree to which
 the individual $x$ satisfies the property characterized by
 $(b,d)$.

 #+begin: example-src :filename "LingExamples.hs" :defn grade
 sampleGrade = do
   v ‚Üê sampleNormedVector      -- reference vector for the grade
   b ‚Üê sample (Gaussian 0 1)   -- reference bias
   return (\x ‚Üí (b + x ¬∑ v))
 #+end:

#+HEADER: :file grade.svg :imagemagick yes
#+HEADER: :results output silent :headers '("\\usepackage{tikz}" "\\usetikzlibrary{arrows,calc,intersections}")
#+HEADER: :fit yes :imoutoptions -geometry 400 :iminoptions -density 600
#+BEGIN_src latex
     \begin{tikzpicture}[scale=2]
       \draw[->] (-3.5,0)--(3.5,0) node[right]{\Large $x$};
       \draw[->] (0,-2.5)--(0,2.5) node[above]{\Large $y$};
       \path [name path=boundary] (-3,-2) rectangle (3,2);
       \clip (-3,-2) rectangle (3,2);
       \foreach \i in {-3,...,4}
       {
         \draw [name path=isotall] (-2+\i,-4) -- (1+\i,4);
         % \draw [name intersections={of=boundary and isotall}] (intersection-2) -- (intersection-1) node [anchor=west]  {$\i$}; % destroys the drawing 
       }
       \fill[color=blue,opacity=0.15] (-2,-4) -- (1,4) -- (10,4) -- (10,-4);
  \end{tikzpicture}
#+END_src



                 [[./grade.svg]]

**** Idea 2
 The degree to which an individual $x$ satisfies a property
 characterized by a box centered at \(c\) and of dimensions \(d\) is
 given by $s(x)$.

 The characteristic vectors c and d are sampled from the vector
 space. Thus we get:


 #+begin_src haskell
 grade :: P Grade
 grade = do
   c ‚Üê sample (Gaussian 0 1)
   d ‚Üê sample (Gaussian 0 1)
   let s x = 1 - max [abs(x·µ¢ - c·µ¢) / d·µ¢ | i ‚Üê [1..n]]
   return s
 #+end_src

 This definition entails that the subspace
 corresponding to a predicate coincides with the space where its degree
 of satisfaction is positive.

 Remarks:
   - $s(x) = 1$ iff. $x$ is at the center of the box.
   - $s(x) > 0$ iff. $x$ is inside the box.

**** Example

 That is, if we observe that ‚ÄúJohn is taller than Mary‚Äù, we will infer
 that ‚ÄúJohn is tall‚Äù is slightly more probable than ‚ÄúJohn is not
 tall‚Äù.

 #+begin: example-src :filename "LingExamples.hs" :defn exampleTall
 exampleTall = do
   tall ‚Üê sampleGrade
   john ‚Üê sampleInd
   mary ‚Üê sampleInd
   observe (more tall john mary)
   return (is tall john)
 #+end:

** Subsective Graded Adjectives

*** Reminder: intersective vs subsective adjectives

We could interpret "socrates is a large man", as |man socrates ‚àß large
socrates|. This is what we call an intersective interpretation.

However, this can pose problems. Consider the following set of statements:

- Dumbo is not a large elephant
- Mickey is a large mouse
- Dumbo is larger than Mickey

If Dumbo is not large, but Mickey is large, so how could Dumbo be
larger than Mickey? The intuitive answer is that when we say "Mickey
is a large mouse", we mean that Mickey is large *for* a mouse. But he
can still be small compared to Dumbo (who is not a mouse)!


*** Probabilistic interpretation of subsective adjectives

As a way forward, we are going to give an interpretation of ‚Äúmickey is
large for a mouse‚Äù, by looking for the average size of a mouse, and
check that the size of mickey is greater than that.

#+begin: example-src :filename "LingExamples.hs" :defn subsectiveIs
subsectiveIs g cn x = g x > averageFor g cn
#+end:

and:

#+begin: example-src :filename "LingExamples.hs" :defn averageFor
averageFor g cn = expectedValue <$> mcmc 1000 sampleGForCn
  where sampleGForCn = do
          y ‚Üê sampleSome cn
          return (g y)
#+end:

And we can now interpret meaningfully  inference problems such as:

 - Dumbo is not a large elephant
 - Mickey is a large mouse
 - Most elephant is larger than most mice
 - $‚ä®$? Dumbo is larger than Mickey

We do so as follows:

#+begin: example-src :filename "LingExamples.hs" :defn exampleDumbo
exampleDumbo = do
  elephant ‚Üê samplePredicate
  mouse ‚Üê samplePredicate
  dumbo ‚Üê sampleSome elephant
  mickey ‚Üê sampleSome mouse
  large ‚Üê sampleGrade
  observe (not <$> (subsectiveIs large elephant dumbo)) 
  observe (subsectiveIs large mouse mickey)
  observe (most mouse (\x ‚Üí most elephant (\y ‚Üí more large y x)))
  return (more large dumbo mickey)
#+end:

* Computations: MCMC, Gradients
Link to video lecture: https://youtu.be/ED4i5cF6nPk
  So far, we can:
- evaluate natural language problems to probabilistic programs
- evaluate the expected value of such programs as mathematical formulas

In this chapter we'll learn how to evaluate such formulas to numbers.

** Exact evaluation

Represent the meaning of a probabilistic program as a list of pairs: \((outcome,probability)\).

Then:

#+begin_src Haskell
‚ü¶return k‚üß    =   [(k,1)]
‚ü¶x ‚Üê t; u‚üß    =   [(y,p√óq) | (x,p) ‚àà ‚ü¶t‚üß, (y,q) ‚àà ‚ü¶u(x)‚üß]
‚ü¶sample d‚üß    =   [(x,P_d(x)) | x ‚àà Œ©]
‚ü¶observe œÜ‚üß   =   [(‚óá,Indicator(œÜ))]
#+end_src

This yields an exhaustive list of all possible outcome (a new finite distribution).

See haskell implementation here: file:Exact.hs

But, this cannot work for continuous distributions!

*** Example: drug test

    file:DrugTest.hs
   
** Continuous case

- The formulas we got in the previous chapters involve integrals which
  are typically not easy to compute (when involving non-trivial
  spaces).

Fortunately there are ways to approximate probabilities directly
without resorting to symbolic integration.

** Markov Chain Monte Carlo
*** Monte Carlo methods:

Assume a probabilistic program \(t\) returning a value in [0,1].
To evaluate \(ùîº(t)\):
 - ~n := 0; q := 0~
 - repeat:
   - take a random output \(x\) of \(t\) by sampling from \(t\)
     - !! If the output is discarded (according to observe), then try again
   - ~n := n + 1~
   - ~q := q + x~

After sufficiently many trials:

 - \(ùîº(t) ‚âà  \frac q n\)

*** Markov Chain
- Informally: ‚Äúa random walk‚Äù
- assume set of states \(S\), and a starting state \(s_i\).
- for each pair of states \((s,t)\), assume a probability \(P(s,t)\) to
  transition from \(s\) to \(t\).
- at each step, transition from a state to another according to the
  given probabilities.
- an interesting question: after an infinite number of steps, what is
  the probability to end at a given state \(s_f\)?
  + if \(T\) is the transition matrix and \(s_i\) the initial state, $T^\infty s_i$

*** MCMC

Sampling in complicated probabilistic program \(t\) is not so
easy. Typically we have a space with a number of dimensions (the Cartesian
product of a number of distributions), and a complicated filtering function
(we can have an \(observe\) statement which depends on a complicated
condition.)

One way to improve on this method is to *define* a Markov Chain where:
- each state is an element of the space sampled by \(t\).
  - for example if we sample two variable \(x\) and \(y\), \((x,y)\) will be one of these possible states.
  - The probabilistic program gives a probability for the final outcome of every state. Say \(P_t(x,y)\).

- We can setup the random walk between states so that it's probable to
  walk from a state \((x,y)\) to a state \((x',y')\), if \(P_t(x',y')\) is
  larger \(P_t(x,y)\)

- This way, after many steps, we are likely to be on a probable state.

- Additionally, once we have found a possible state (one which passes
  all ~observe~ statements) we can find a new state by taking one step
  of the random walk. There is no need to "start from scratch" as in
  the naive Monte Carlo approach.


We use this kind of random walk to sample in \(t\), and apply the Monte
Carlo method as usual to evaluate the proportion.

Potential issues:
- Defining the walk is not too easy when there are many variables
  (also, with if statements (and |observe|), the existence of
  variables depend on the value of others.)
- You never find a valid \(x‚ààt\) to start with
- The space is divided in regions which are not connected, or a walk
  from one to the other is highly improbable.

*** The Structure of the probabilistic program influences performance
In any program portion |x ‚Üê t; observe œÜ(x)| a potential pitfall is to chose $t$ too wide, (eg. \(x\)
could be a tuple of many independent variables),
followed by a very restrictive $œÜ$. In such situation the Monte Carlo
algorithm will spend a lot of time sampling elements of $t$ only to
discard them. It is better to restrict $t$ to one of its subspaces $u$
so that $œÜ$ becomes more easy to satisfy on $u$.
*** Inner evaluation of proportions
When using quantifiers, we evaluate more proportions/measures, and an
inner instance of the MCMC algorithm must be employed. This can be
very slow! A potential way out: when we use boxes some integrals can
be computed symbolically and we save much resources.

** More methods!

Any mixture of the above is thinkable. (Sample and do
gradient descent to determine most probable value of the samples, etc.)

** Returning to our motivating example

   file:Balls.hs
*** Choice of prior (again)
The (Uniform 0 1) prior is biasing the result towards \(1/2\). In order
not to bias the result, one should use the Jeffrey's prior, which in
this case is \(Beta(0.5, 0.5)\).

The Jeffrey's prior is given by the square root (the determinant of)
the Fisher information (matrix) I. (I = variance of the derivative of
log of density.)

** More Probabilistic programming packages
- STAN: https://mc-stan.org/
- WebPPL: http://dippl.org/
- And many, many, many papers about industrial strength and idealized probabilistic
  programming languages.

* Test suite for Probabilistic Inference, Outlook
** Building a corpus of probabilistic inference
   Data driven approaches need corpora to learn. But, a corpus is also
   important for testing systems to establish their strengths,
   weakness, robustness, etc.  For natural language inference, there
   are several corpora, including FraCaS and SICK.  However, apart
   from a corpus designed in CLASP, no corpus exists for probabilistic
   inference problems.
*** Generalized quantifiers
    Quantification with most, few, many, several, etc. give rise to non-logical inference.
    While in building a testsuite for probabilistic inference they are irreplaceable,
    we should limit ourselves not to overuse them as their complex semantic
    nature does not make easily available various meanings that can be obtained in the case 
    of standard, universal quantification.
    - Every man has a car. (Not too hard to comprehend its meaning albeit ambiguous)
    - Every man can drive every car. (Not too hard to understand)
    - Most men can drive most cars. (What does that mean exactly?)
    - Many men love fast cars but not always can afford them. (always - refers to many men?: TODO check English)
*** Bare plurals, Generics, and Indefinite Noun phrases

    One has to be careful when dealing with bare plurals as they have (at least) two kinds of meanings:
    They serve as generalized quantifiers (like most) and they also may carry generic meaning
    - Ducks lay eggs. (Generic because it's not equivalent to Most ducks lay eggs or all ducks lay eggs)
    - Small ducks do not trust humans. (Generalize quantifier usage ~ most small ducks don't trust humans)
    - Honeybees produce beeswax. (Generic because, for example, queens do not produce any beeswax)  
      
    While indefinite noun phrases in Montague Grammar are translated with the help of existential quantifier,
    they also exhibit traits of generic readings:
    
    - A lion has a mane. (Generic: only adult male lions do)
    - An adult male lion has a mane. (~ All typical adult male lions have manes)
    - A lion saw a zebra.

*** Adverbs of frequency
    Adverbs of frequency, e.g., seldom, frequently, rarely, couple of times per day, etc. 
    Context dependent: 
    - John frequently goes to Paris 
    - John frequently takes French classes 
    - John frequently gets distracted during a lecture 
    - John frequently sips water during a day
    
    Adverbs of frequency interact with quantifiers.
*** Adjectives and Comparatives derived from them
    Consider:
    Few people are basketball players.
    Basketball players are taller than most non basketball players.
    John is a basketball player.    
    )===>
    John is taller than many people.
    
    Consider another example:
    
    John is taller than Mary.
    Mary is taller than Bob.
    )===>
    John is taller than Bob.

    Another questions: Is John tall?

** Evaluation criteria of a probabilistic inference system against the corpus
   How to evaluate an inference, on what scale: Yes‚à£No, Yes‚à£NA‚à£No, Yes‚à£Kind of Yes‚à£NA‚à£Kind of No‚à£No, etc.
   Our scale: [0,1]
   Valid inference has a probability 0.5+x
   Invalid inference has a probability 0.5-y 

   Most vampires are sleepy.
   Dracula is a vampire.
   )===>
   Dracula is sleepy.     (Probable with d1)

   Almost all vampires are sleepy.
   Dracula is a vampire.
   )===>
   Dracula is sleepy.     (Probable with d2)

   Question: If Almost all > Most, should it be that d2>d1?

     
   Question: d1 VS d3? d2 VS d3?


   All vampires are sleepy.
   Dracula is probably a vampire.
   ------------------------------
   Dracula is sleepy.     (Probable with d4) 
   
   Question: d1 VS d4? d2 VS d4? d3 VS d4?

   In general, if we define a notion of monotonicity in premises, 
   would our notions of inference be compatible with them?
   
** Probabilistic approach to pragmatics
*** The Rational Speech Act (RSA) model
    A popular approach to pragmatics is the RSA model. There is a
    whole ESSLLI course on this! We won't get into the details, but a
    quick outline is the following.

RSA assumes two agents, a listener \Li{} and a speaker \Spk{}. \Spk{}
utters a declarative sentence \(u\) heard by \Li{}, without
transmission error. The point of RSA is to model how, assuming Gricean
cooperativeness between \Spk{} and \Li{}, \Li{} should disambiguate
among possible interpretations of \(u\).

1. Does u literally mean œÜ? For this we may use the model described
   above. (The uncertainty over the meaning of the utterance u is
   represented by a parameter Œ∏.)

  \(P_{L‚ÇÄ}(œÜ   ‚à£ u)   = ùîº_{Œ∏ ‚àà Parameters}[‚ü¶u‚üß·∂ø ‚ä¢ œÜ]\)

2. The (Gricean) speaker chooses $u$ to (soft-)maximize the probability
   that L understands œÜ:

  \(P_{S‚ÇÅ}(u   ‚à£ œÜ)   ‚àù (P_{L‚ÇÄ}(œÜ   ‚à£ u) / C(u))^{Œ±}\)                  

3. The (pragmatic) speaker considers the meaning of u in proportion to
   the estimated choices of the (Gricean) speaker.

  \(P_{L‚ÇÅ}(œÜ   ‚à£ u)   ‚àù P_{S‚ÇÅ}(u ‚à£ œÜ)  √ó P(œÜ)\)


Take away message: you can use an RSA layer *on top of* our
probabilistic models.

*** Numbers

    S: ‚ÄúI ate 5 cookies‚Äù.

    How many cookies did S eat?

    ‚ü¶I ate n cookies‚üß = ‚àÉm ‚â• n  eat_cookies(I, m)

    If we consider only 3 possible utterances and meanings (say, there
    were 7 cookies in the box), we get the literal probability
    assignment \(P_{L‚ÇÄ}(œÜ ‚à£ u)\):

    | Utterance       | ate(5) | ate(6) | ate(7) |
    |-----------------+--------+--------+--------|
    | I ate 5 cookies |    1/3 | 1/3    | 1/3    |
    | I ate 6 cookies |      0 | 1/2    | 1/2    |
    | I ate 7 cookies |      0 | 0      | 1      |

softmax by column (Œ±=4), assuming all utterances have the same cost,
yields \(P_{S‚ÇÅ}(u ‚à£ œÜ)\):

    | Utterance       | ate(5) | ate(6) | ate(7) |
    |-----------------+--------+--------+--------|
    | I ate 5 cookies |      1 |   0.16 |   0.01 |
    | I ate 6 cookies |      0 |   0.84 |   0.06 |
    | I ate 7 cookies |      0 |      0 |   0.93 |

Normalizing by row yields \(P_{L‚ÇÅ}(œÜ   ‚à£ u)\)

    | Utterance       | ate(5) | ate(6) | ate(7) |
    |-----------------+--------+--------+--------|
    | I ate 5 cookies |   0.85 |   0.14 |   0.01 |
    | I ate 6 cookies |      0 |   0.93 |   0.07 |
    | I ate 7 cookies |      0 |      0 |      1 |


Exercises:
 - iterate the process (to get a level-2 pragmatic listener)
 - Repeat everything with Œ±=‚àû
 - Consider the utterance ‚ÄúI ate 2 cookies‚Äù. 

We won't further discuss the merits of this model.

In the above we have:

  1. used a ‚Äústrict‚Äù probabilistic semantics
  2 and laid a pragmatic component on top of it.

BUT we can also *bake in* pragmatics into the /raw/ probabilistic meaning.

For example,
‚ü¶I ate n cookies‚üß = eat_cookies(I, m) where m is chosen in a Poisson distribution with mode n.

*** The proviso problem                                              :Sandro:
- If John goes to the sea he will take his cat with
  him. (Presupposition is that the cat and going to the sea are not related;
  so, John definitely has a cat.)
- If John goes to the sea then he will take his scuba. (Presupposition
  is that the scuba and the sea are related; so John doesn't need to
  have a cube, but he might get one for this purpose.)

  How to model this? Lassiter (2012) proposes to use
  probabilistic approach for that, which we adopt in our settings.
  Assume we have a candidate presupposition.  If the candidate
  presupposition œÄ is not probabilistic ally inferrable from the initial
  sentence, then we consider œÄ to be an actual
  presupposition. Otherwise, œÄ doesn't qualify for a presupposition.

* References

- Variational Inference: A Review for Statisticians (by David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe)
  https://arxiv.org/pdf/1601.00670.pdf 

- Variational Inference (lecture notes by David M. Blei)
https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf

- A Tutorial on Variational Bayesian Inference
  http://www.orchid.ac.uk/eprints/40/1/fox_vbtut.pdf

- Bayesian inference
  https://en.wikipedia.org/wiki/Bayesian_inference#Bayesian_inference

- Dealing with observing zero-measure events
  Paradoxes of Probabilistic Programming And How to Condition on Events of Measure Zero with Infinitesimal Probabilities
  Jules Jacob, 2021

- Presuppositions, provisos, and probability
  Semantics & Pragmatics Volume 5, Article 2: 1‚Äì37, 2012
  Lassiter, Daniel
  https://semprag.org/index.php/sp/article/view/sp.5.2/pdf

- PROBABILISTIC INFERENCE AND THE CONCEPT OF TOTAL EVIDENCE
  J. Hintikka & P. Suppes (Eds), Aspects of Inductive Logic, Amsterdam: North-Holland, 1966, pp. 49-65
  PATRICK SUPPES 
  https://suppescorpus.sites.stanford.edu/sites/g/files/sbiybj7316/f/probabilistic_inference_and_the_concept_of_total_evidence_71.pdf

** Our references

- A Compositional {Bayesian} Semantics for Natural Language
  JP Bernardy, R Blanck, S Chatzikyriakidis, S Lappin
  Proceedings of the First International Workshop on Language Cognition and Computational Models

- Bayesian inference semantics: A modelling system and a test suite
  Jean-Philippe Bernardy, Rasmus Blanck, Stergios Chatzikyriakidis, Shalom Lappin, Aleksandre Maskharashvili
  Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (* SEM) 2019

- Predicates as Boxes in Bayesian Semantics for Natural Language
  JP Bernardy, R Blanck, S Chatzikyriakidis, S Lappin, A Maskharashvili
  Proceedings of the 22nd Nordic Conference on Computational Linguistics, 333-337

- Jean-Philippe Bernardy, Rasmus Blank, Aleksandre Maskharashvili, 
  ‚ÄúA Logic with Measurable Spaces Fornatural Language Semantics‚Äù,
  AMIM Vol.25 No.2, 2020, pp. 31-43

- Julian Grove, Jean-Philippe Bernardy, Stergios Chatzikyriakidis
  From compositional semantics to Bayesian pragmatics via logical inference
  NALOMA 2021




# Local Variables:
# ispell-local-dictionary: "american"
# End:



#  LocalWords:  xelatex polycode fmt mathbin LaTeX SumInt usepackage
#  LocalWords:  DeclareMathOperator tikz usetikzlibrary calc fadings
#  LocalWords:  automata unicode newcommand ensuremath Spk fontspec
#  LocalWords:  setmainfont Libertinus setsansfont setmathfont Sumint
#  LocalWords:  stackengine ensurestackMath stackinset displaystyle
#  LocalWords:  stackanchor mathchoice ooalign cr hidewidth svg nd cd
#  LocalWords:  latexpreview Aleksandre Maskharashvili ProbInfer src
#  LocalWords:  ESSLLI ghci DiceExample hs doit logics Gottlob Frege
#  LocalWords:  th mary ary pluto joe math formalizing morpho Covid
#  LocalWords:  rejectable instantiations Muggsy Bogues Sandro dx iff
#  LocalWords:  overloadings equiprobable areaRoom frac multline sqrt
#  LocalWords:  DiscreteUniform ballBlue mathrm isBlue blueBall defn
#  LocalWords:  twoBallsAtLeastOneBlue twoDieAbove DrugTest isUser ok
#  LocalWords:  exampleDrug testedPositive exampleBalls priori testEq
#  LocalWords:  boolToColor redBall integrand integrators Sloubi Ph
#  LocalWords:  testPositive Montagovian de Groote CN Quant socrates
#  LocalWords:  LingExamples exampleSocrates samplePredicate premiss
#  LocalWords:  Montegovian sampleInd CNs sampleVectorOf sampleable
#  LocalWords:  cardinalities Ie predicateSimple sampleNormedVector
#  LocalWords:  imagemagick imoutoptions iminoptions tikzpicture Pred
#  LocalWords:  xshift textit cn sampleSome atLeast atMost vp mathit
#  LocalWords:  exampleBirds yshift foreach xtext ytext filldraw HOL
#  LocalWords:  forAll stochastically computable dualizing Eg isotall
#  LocalWords:  sampleGrade exampleTall Subsective intersective mcmc
#  LocalWords:  subsective subsectiveIs averageFor expectedValue eg
#  LocalWords:  sampleGForCn exampleDumbo infty subspaces WebPPL RSA
#  LocalWords:  FraCaS testsuite monotonicity Gricean softmax Blei
#  LocalWords:  Lassiter inferrable Variational Kucukelbir McAuliffe
#  LocalWords:  Hintikka Suppes Blanck Chatzikyriakidis Lappin
#  LocalWords:  modelling
